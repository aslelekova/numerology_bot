# services/question_service.py
from aiogram.fsm.context import FSMContext
from services.gpt_service import client


async def generate_question_response(question: str, user_name: str, birth_date: str, state: FSMContext) -> str:
    user_data = await state.get_data()
    response_text = user_data.get('response_text')
    prompt = (
        f"–ú–µ–Ω—è –∑–æ–≤—É—Ç {user_name}, –º–æ—è –¥–∞—Ç–∞ —Ä–æ–∂–¥–µ–Ω–∏—è {birth_date}. "
        f"–Ø —Ö–æ—á—É –∑–∞–¥–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –≤–æ–ø—Ä–æ—Å: {question}. –û—Ç–≤–µ—Ç—å –Ω–∞ –Ω–µ–≥–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å–∫–ª–∞–¥–∞ {response_text}, "
        f"–∏ –ø–æ—Å—Ç–∞—Ä–∞–π—Å—è, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç –±—ã–ª –Ω–µ –¥–ª–∏–Ω–Ω–µ–µ 600 —Å–∏–º–≤–æ–ª–æ–≤."
    )
    
    response_text = await generate_response(prompt)
    return response_text


async def generate_response(prompt: str) -> str:
    messages = [{"role": "user", "content": prompt}]
    
    response = await client.chat.completions.create(
        messages=messages,
        model="gpt-4o-2024-08-06"
    )
    
    return response.choices[0].message.content


async def generate_suggestions(user_question: str) -> str:
    suggestion_prompt = (
        f"–ü—Ä–µ–¥–ª–æ–∂–∏ —Ç—Ä–∏ –Ω–æ–≤—ã—Ö –≤–æ–ø—Ä–æ—Å–∞, –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∞ –≤–æ–ø—Ä–æ—Å–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: '{user_question}'. "
        f"–ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø—Ä–æ–Ω—É–º–µ—Ä–æ–≤–∞–Ω —Ç–∞–∫–æ–π –∫–æ–º–±–∏–Ω–∞—Ü–∏–µ–π \"- üîÆ\""
        f"–ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏."
        f"–°—Ç–∞—Ä–∞–π—Å—è, —á—Ç–æ–±—ã –≤–æ–ø—Ä–æ—Å—ã –±—ã–ª–∏ —è—Å–Ω—ã–º–∏ –∏ –ª–æ–≥–∏—á–Ω—ã–º–∏."
    )

    messages = [{"role": "user", "content": suggestion_prompt}]
    

    suggestion_response = await client.chat.completions.create(
        messages=messages,
        model="gpt-4o-2024-08-06",
        max_tokens=150
    )
    
    return suggestion_response.choices[0].message.content
